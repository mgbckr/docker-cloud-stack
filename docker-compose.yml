version: "3.7"

#
#
# # Notes
#
#
# ## HDFS networking / docker proxy issue
# 
# When using virtual IPs, all requests from a service seem to come from the proxy (-endpoint; usually on x.x.x.2) 
# which is spawned by Docker to handle those virtual IPs, I guess.
# Now, when a HDFS datanode is on IP 10.0.0.4 and tries to register with a HDFS namenode, 
# then this namenode will still see it as 10.0.0.2 (the proxy/endpoint address).
# When there are several datanodes this results in datanodes being constantly added and removed because the namenode
# thinks it is one an the same namenode talking to it. This probably is a BUG in Docker (I think 
# I saw a bug entry somewhere but can't find it anymore)! 
#
# To solve this we switched from virtual IPs to DNS round robin (endpoint_mode: dnsrr) and setting replica: 1.
# This way each datanode has its own IP and things should work out fine, I think :)
# 
# A note on replication for datanodes: this probably does not work at all because clients of HDFS need to know the specific datanode
# they are talking to. Thus load balancing via virtual IPs or DNS round robin would kill the protocol.
# See: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html
#
# 
# ## Docker: config versioning
# 
# The name property is not documented, as far as I can tell and onlye available for docker-compose versions 3.5+. 
# See here for how to use them: https://github.com/moby/moby/issues/35048#issuecomment-424372653
#
#
# ## Docker: overriding and extending docker-compose.yml files
#
# Interesting to switch between development, testing and production  
# https://docs.docker.com/compose/extends/#understand-the-extends-configuration
#
#
# ## Kafka
# Kafka needs running Zookeeper instances. Thus we added a `sleep 10` after starting Zookeeper and before firing up Kafka. 
# However we should probably do something more sophisticated I guess. 
# One approach would be to increase the zookeeper.connection.timeout.ms" config entry. 
# However without the sleep this caused Kafka to complain about already existing ephermal nodes, ie.e., along the lines of:
# ```
# Error while creating ephemeral at /brokers/ids/1, node already exists and owner '216186131422332301' does not match current session '288330817911521280' (kafka.zk.KafkaZkClient$CheckedEphemeral)
# ```
# For now we will stick to the sleep since it simply works :).
#
# Pointers:
# * https://github.com/voxpupuli/puppet-kafka/issues/81
# * https://stackoverflow.com/questions/35613345/kafka-startup-fails-with-zookeeper-timeout-remote-server-yet-the-machine-can
# * https://kafka.apache.org/documentation.html
# * https://issues.apache.org/jira/browse/KAFKA-7165
#
# ## Accumulo notes
#
# cd accumulo; apt update; apt install -y make; apt install -y g++; ./bin/bootstrap_config.sh; 
# add `$HADOOP_PREFIX/share/hadoop/common/[^.].*.jar,` to `detachgeneral.classpaths` in `conf/accumulo-site.xml`
#
# TODO: adjust swapiness to <=10 says Accumulo:
# WARN : System swappiness setting is greater than ten (60) which can cause time-sensitive operations to be delayed. Accumulo is time sensitive because it needs to maintain distributed lock agreement.
#
#
# ## General
#
# ### TODO: GRACEFUL SHUTDOWN; as it is data may be lost!!! Currently everything is killed
# On Docker swarm this actually won't work. When using docker stack rm, then the network will be killed first. Thus communication is impossible:
# https://github.com/docker/swarm/issues/2897
# We have to work around this. Maybe to local shutdowns, wait for a fix of the above issue, or kill anybody that uses docker stack rm on this stack. 
#
#

configs:
  manage.sh:
    name: manage.sh-${CONFIG_VERSION:-0}
    file: ./assets/manage/manage.sh
  id_rsa:
    name: id_rsa-${CONFIG_VERSION:-0}
    file: ./assets/manage/config/id_rsa
  id_rsa.pub:
    name: id_rsa.pub-${CONFIG_VERSION:-0}
    file: ./assets/manage/config/id_rsa.pub
  # for debugging
  run.sh:
    name: run.sh-${CONFIG_VERSION:-0}
    file: ./assets/run.sh

networks: 
  hadoop:
    attachable: true

services:
  node-master-1:
    hostname: node-master-1
    image: docker-cloud-stack:latest
    stop_grace_period: 3m
    depends_on: 
      - node-worker-1
      - node-worker-2
      - node-worker-3
    deploy:
      endpoint_mode: dnsrr
      # placement:
      #   constraints:
      #     - node.hostname == HOSTNAME
    configs:
      - source: manage.sh
        target: /app/manage.sh
      - source: id_rsa
        target: /home/node-docker/.ssh/id_rsa
      - source: id_rsa.pub
        target: /home/node-docker/.ssh/id_rsa.pub
      - source: id_rsa.pub
        target: /home/node-docker/.ssh/authorized_keys
      # debugging
      - source: run.sh
        target: /app/run.sh
    volumes:
      # config
      - ./assets/manage/config:/app/config
      # hdfs and kafka
      - /tmp/docker/node-master-1/data/a:/app/data/a
      - /tmp/docker/node-master-1/data/b:/app/data/b
      - /tmp/docker/node-master-1/data/c:/app/data/c
      # zookeeper
      - /tmp/docker/node-master-1/data/zookeeper:/app/data/zookeeper
      # logs
      - /tmp/docker/node-master-1/logs:/app/logs
    command: >
      bash /app/run.sh master
    # ports:
    #   - "9000:9000"
    networks:
      - hadoop

  node-worker-1:
    hostname: node-worker-1
    image: docker-cloud-stack:latest
    stop_grace_period: 3m
    deploy:
      endpoint_mode: dnsrr
      # placement:
      #   constraints:
      #     - node.hostname == HOSTNAME
    configs:
      - source: id_rsa.pub
        target: /home/node-docker/.ssh/authorized_keys
      # debugging
      - source: run.sh
        target: /app/run.sh
    volumes:
      # hdfs and kafka
      - /tmp/docker/node-worker-1/data/a:/app/data/a
      - /tmp/docker/node-worker-1/data/b:/app/data/b
      - /tmp/docker/node-worker-1/data/c:/app/data/c
      # zookeeper
      - /tmp/docker/node-worker-1/data/zookeeper:/app/data/zookeeper
      # logs
      - /tmp/docker/node-worker-1/logs:/app/logs
    command: >
      bash /app/run.sh worker 1
    networks: 
      - hadoop

  node-worker-2:
    hostname: node-worker-2
    image: docker-cloud-stack:latest
    stop_grace_period: 3m
    deploy:
      endpoint_mode: dnsrr
      # placement:
      #   constraints:
      #     - node.hostname == HOSTNAME
    configs:
      - source: id_rsa.pub
        target: /home/node-docker/.ssh/authorized_keys
      # debugging
      - source: run.sh
        target: /app/run.sh
    volumes:
      # hdfs and kafka
      - /tmp/docker/node-worker-2/data/a:/app/data/a
      - /tmp/docker/node-worker-2/data/b:/app/data/b
      - /tmp/docker/node-worker-2/data/c:/app/data/c
      # zookeeper
      - /tmp/docker/node-worker-2/data/zookeeper:/app/data/zookeeper
      # logs
      - /tmp/docker/node-worker-2/logs:/app/logs
    command: >
      bash /app/run.sh worker 2
    networks: 
      - hadoop

  node-worker-3:
    hostname: node-worker-3
    image: docker-cloud-stack:latest
    stop_grace_period: 3m
    deploy:
      endpoint_mode: dnsrr
      # placement:
      #   constraints:
      #     - node.hostname == HOSTNAME
    configs:
      - source: id_rsa.pub
        target: /home/node-docker/.ssh/authorized_keys
      # debugging
      - source: run.sh
        target: /app/run.sh
    volumes:
      # hdfs and kafka
      - /tmp/docker/node-worker-3/data/a:/app/data/a
      - /tmp/docker/node-worker-3/data/b:/app/data/b
      - /tmp/docker/node-worker-3/data/c:/app/data/c
      # zookeeper
      - /tmp/docker/node-worker-3/data/zookeeper:/app/data/zookeeper
      # logs
      - /tmp/docker/node-worker-3/logs:/app/logs
    command: >
      bash /app/run.sh worker 3
    networks: 
      - hadoop

    